{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chat_models import init_chat_model\n",
    "#model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "#from langchain_core.messages import HumanMessage\n",
    "#model.invoke([HumanMessage(content=\"Hi! I love mangoes\")])\n",
    "#model.invoke([HumanMessage(content=\"What fruit do I like?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bc7fd",
   "metadata": {},
   "source": [
    "# üß∞ Step 1: Install Ana‚Äôs Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core langgraph>0.2.27\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# üîë Load your OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537363b",
   "metadata": {},
   "source": [
    "# ü§ñ Step 2: Say Hello to¬†Ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f72b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Mangoes are delicious, I agree! Do you have a favorite way to enjoy them?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# üß† Load the OpenAI model (GPT-3.5 or GPT-4)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # üí¨ Chat-capable model\n",
    "\n",
    "# üíå Send a message to the model\n",
    "response = llm.invoke([HumanMessage(content=\"Hi! I love mangoes\")])  # ü•≠\n",
    "\n",
    "# üì§ Print the AI's response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a5c567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure, can you tell me what your favorite fruit is?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke([HumanMessage(content=\"What fruit do I like?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963170c",
   "metadata": {},
   "source": [
    "# üß† Step 3: Give Ana a Memory with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a61c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# üß† Define your model step (Ana's thinking process)\n",
    "def call_model(state):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "# üï∏Ô∏è Build the LangGraph workflow\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# üß© Add a node named 'model1' for the LLM call\n",
    "workflow.add_node(\"model1\", call_model)\n",
    "\n",
    "# üîÅ Define the flow: start ‚Üí model1\n",
    "workflow.add_edge(START, \"model1\")\n",
    "\n",
    "# üíæ Enable memory so Ana remembers past messages\n",
    "memory = MemorySaver()\n",
    "ana = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03980a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßµ Define a unique user thread to track conversation history\n",
    "config = {\"configurable\": {\"thread_id\": \"user1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2bc1d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi Ana, I love mangoes!', additional_kwargs={}, response_metadata={}, id='815dcee9-41a1-4405-a9bc-5b0e549553e1'),\n",
       "  AIMessage(content=\"Hi there! That's great to hear, mangoes are definitely a delicious fruit. Do you have a favorite way to enjoy them?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 15, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BnWQkifzOzW9i743KvmTToa5pXfJG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e697771c-bdce-4fd5-a023-dab7c40b9754-0', usage_metadata={'input_tokens': 15, 'output_tokens': 27, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='What fruit do I like?', additional_kwargs={}, response_metadata={}, id='981b8f10-882c-4669-939b-67bdca41dfd9'),\n",
       "  AIMessage(content='You mentioned earlier that you love mangoes, so it seems like mangoes are your favorite fruit! Do you enjoy eating them fresh, in smoothies, or in any particular dish?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 56, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BnWQl2W0f7ABpwcS2sl3j6U1riUsf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--554e963c-46c5-4674-9ef0-838ec6cf92ab-0', usage_metadata={'input_tokens': 56, 'output_tokens': 37, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# üó®Ô∏è First message: user tells Ana they love mangoes\n",
    "ana.invoke({\"messages\": [HumanMessage(\"Hi Ana, I love mangoes!\")]}, config)\n",
    "\n",
    "# üß† Second message: Ana should remember your preference!\n",
    "ana.invoke({\"messages\": [HumanMessage(\"What fruit do I like?\")]}, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33800bc8",
   "metadata": {},
   "source": [
    "# üßô Step 4: Add a Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a24175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi Ana! How are you?', additional_kwargs={}, response_metadata={}, id='63661139-63e0-495e-8003-b4bd018892f3'),\n",
       "  AIMessage(content=\"Hello! I'm here and ready to help with anything cooking-related. What would you like to know or talk about today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 30, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BnWUWuNj2FHrOm9xcAxuyee6kxMEf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5a0f5a80-e197-4c60-b787-07d546208e5a-0', usage_metadata={'input_tokens': 30, 'output_tokens': 25, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# üç≥ Define Ana's personality: a friendly cooking assistant!\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly cooking assistant. Always say something about cooking\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "# üß† LLM call with updated prompt\n",
    "def call_model(state):\n",
    "    prompt = prompt_template.invoke({\"messages\": state[\"messages\"]})\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "# üõ†Ô∏è Build LangGraph workflow\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_node(\"model2\", call_model)\n",
    "workflow.add_edge(START, \"model2\")\n",
    "\n",
    "# üíæ Enable memory\n",
    "memory = MemorySaver()\n",
    "ana = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# üßµ Set user thread for stateful memory\n",
    "config = {\"configurable\": {\"thread_id\": \"user2\"}}\n",
    "\n",
    "# üí¨ Test Ana's personality\n",
    "ana.invoke({\"messages\": [HumanMessage(\"Hi Ana! How are you?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f41348",
   "metadata": {},
   "source": [
    "# üåç Step 5: Make Ana Speak Any¬†Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79eceb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='‡§∏‡•á‡§¨ ‡§è‡§ï ‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§´‡§≤ ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§ü‡§æ‡§Æ‡§ø‡§® ‡§∏‡•Ä, ‡§´‡§æ‡§á‡§¨‡§∞ ‡§î‡§∞ ‡§Ö‡§®‡•ç‡§Ø ‡§™‡•ã‡§∑‡§ï ‡§§‡§§‡•ç‡§µ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•á ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§´‡§æ‡§Ø‡§¶‡•á‡§Æ‡§Ç‡§¶ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§∏‡§ï‡§æ ‡§®‡§ø‡§Ø‡§Æ‡§ø‡§§ ‡§∏‡•á‡§µ‡§® ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡•á‡§π‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 161, 'prompt_tokens': 26, 'total_tokens': 187, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BnWZcyxqIbtQNn6oIjrH6G6NvWjnZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8f5dbe11-f2b1-401b-8443-88369a1ef2e8-0', usage_metadata={'input_tokens': 26, 'output_tokens': 161, 'total_tokens': 187, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'language': 'Hindi'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "# üîë Load your OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# üß† LLM setup (OpenAI model)\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# üßæ Define Ana‚Äôs state with language support\n",
    "class AnaState(TypedDict):\n",
    "    messages: list\n",
    "    language: str\n",
    "\n",
    "# üß∞ Build prompt template with language conditioning\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Respond in {language}. Be helpful.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "# ‚öôÔ∏è LangGraph node logic\n",
    "def call_model(state: AnaState):\n",
    "    prompt = prompt_template.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"language\": state[\"language\"]\n",
    "    })\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# üï∏Ô∏è Build LangGraph workflow\n",
    "workflow = StateGraph(state_schema=AnaState)\n",
    "workflow.add_node(\"model3\", call_model)\n",
    "workflow.add_edge(START, \"model3\")\n",
    "\n",
    "# üíæ Enable memory so Ana remembers past messages\n",
    "memory = MemorySaver()\n",
    "ana = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# üßµ Define user thread (to maintain stateful memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"user3\"}}\n",
    "\n",
    "# üí¨ Invoke Ana in Hindi\n",
    "ana.invoke({\n",
    "    \"messages\": [HumanMessage(\"Ana, what's a healthy fruit?\")],\n",
    "    \"language\": \"Hindi\"\n",
    "}, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a882456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à‡•§ ‡§∏‡§≠‡•Ä ‡§´‡§≤ ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø‡§µ‡§∞‡•ç‡§ß‡§ï ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç, ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§ï‡•Å‡§õ ‡§´‡§≤ ‡§ú‡•à‡§∏‡•á ‡§ï‡•Ä ‡§∏‡•á‡§¨, ‡§Ö‡§Ç‡§ó‡•Ç‡§∞, ‡§∏‡§Ç‡§§‡§∞‡§æ, ‡§Ü‡§Æ ‡§î‡§∞ ‡§™‡§™‡•Ä‡§§‡§æ ‡§ñ‡§æ‡§∏‡§§‡•å‡§∞ ‡§∏‡•á ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§´‡§æ‡§Ø‡§¶‡•á‡§Æ‡§Ç‡§¶ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ø‡•á ‡§´‡§≤ ‡§Ü‡§™‡§ï‡•Ä ‡§°‡§æ‡§á‡§ü ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§ï‡§∞‡§®‡§æ ‡§∏‡•á‡§π‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡§æ‡§≠‡§ï‡§æ‡§∞‡•Ä ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Respond in Hindi. Be helpful.\"\n",
    "user_message = \"Ana, what's a healthy fruit?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea54d8",
   "metadata": {},
   "source": [
    "# üßπ Step 6: Trim Long Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10118499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to your personal information, so I don't know your name. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages\n",
    "\n",
    "# ü§ñ Setup the LLM (assumed defined earlier)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# üßπ 1. Setup message trimmer to limit conversation length\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",            # Keep recent messages only\n",
    "    token_counter=llm,          # Use LLM's tokenizer to count tokens\n",
    "    include_system=True,        # Always include system messages\n",
    "    allow_partial=False,        # Do not allow partial messages\n",
    "    start_on=\"human\",           # Trim starting from last human message\n",
    ")\n",
    "\n",
    "# üìù 2. Define prompt template with dynamic language\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Respond in {language}. Be helpful.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "# ‚öôÔ∏è 3. Define model call that trims messages before invoking LLM\n",
    "def call_model(state: dict):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])  # Trim long history\n",
    "    prompt = prompt_template.invoke({\n",
    "        \"messages\": trimmed_messages,\n",
    "        \"language\": state[\"language\"]\n",
    "    })\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# üîó 4. Setup LangGraph workflow\n",
    "workflow = StateGraph(state_schema=dict)  # Or your AnaState TypedDict\n",
    "workflow.add_node(\"model4\", call_model)\n",
    "workflow.add_edge(START, \"model4\")\n",
    "\n",
    "# üíæ 5. Add memory to persist conversation state\n",
    "memory = MemorySaver()\n",
    "ana = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# üí¨ 6. Sample conversation messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a good assistant.\"),\n",
    "    HumanMessage(content=\"Hi! I'm Ankita.\"),\n",
    "    AIMessage(content=\"Hi there!\"),\n",
    "    HumanMessage(content=\"I love data science.\"),\n",
    "    AIMessage(content=\"That's awesome!\"),\n",
    "    HumanMessage(content=\"Ana, I enjoy working with LangChain.\"),\n",
    "    AIMessage(content=\"LangChain is powerful for building LLM apps.\"),\n",
    "    HumanMessage(content=\"What's a good Python library for data visualization?\"),\n",
    "    AIMessage(content=\"Matplotlib and Seaborn are great options.\"),\n",
    "]\n",
    "\n",
    "# ‚ùì 7. Ask question possibly trimmed out from context\n",
    "config = {\"configurable\": {\"thread_id\": \"user4\"}}\n",
    "query = \"What's my name?\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = ana.invoke({\"messages\": input_messages, \"language\": \"English\"}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed383b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "We discussed Matplotlib and Seaborn as good options for data visualization libraries in Python.\n"
     ]
    }
   ],
   "source": [
    "# ‚ùì 8. Ask question about recent messages (still remembered)\n",
    "config = {\"configurable\": {\"thread_id\": \"ana_user_2\"}}\n",
    "query = \"Which Python libraries did we discuss?\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = ana.invoke({\"messages\": input_messages, \"language\": \"English\"}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfafbdf",
   "metadata": {},
   "source": [
    "# ‚ö° Step 7: Stream Ana's Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d13d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Streaming Ana's response:\n",
      "|Of| course|!| Here|'s| a| fun| science| fact|:| Did| you| know| that| honey| never| spo|ils|?| Archae|ologists| have| found| pots| of| honey| in| ancient| Egyptian| tom|bs| that| are| over| |3|,|000| years| old| and| still| perfectly| edible|.| Honey|'s| high| sugar| content| and| acidity| create| an| environment| that| prevents| the| growth| of| bacteria| and| molds|.| It|'s| pretty| amazing|,| isn|'t| it|?||\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üöÄ Streaming Example: Ana tells a fun fact!\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# üßµ Unique thread to track conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"ana_stream_user_1\"}}\n",
    "\n",
    "# üí¨ User's input question\n",
    "query = \"Hey Ana, can you share a fun science fact?\"\n",
    "language = \"English\"\n",
    "\n",
    "# üì® Add message to input list\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "print(\"üü¢ Streaming Ana's response:\")\n",
    "\n",
    "# üîÑ Stream tokens from Ana as they are generated\n",
    "for chunk, metadata in ana.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",  # üß† Stream by tokens\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # üéØ Filter only model responses\n",
    "        print(chunk.content, end=\"|\")  # ‚å®Ô∏è Print tokens with separator\n",
    "\n",
    "print()  # ‚úÖ Newline after complete response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
